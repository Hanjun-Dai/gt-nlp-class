{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random \n",
    "from collections import defaultdict\n",
    "import xgboost as xgb\n",
    "import gtnlplib.preproc\n",
    "import gtnlplib.constants\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists('data/train_sentences'):\n",
    "    shutil.rmtree('data/train_sentences')\n",
    "os.mkdir('data/train_sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_key = open('data/train_sentences.key', 'w')\n",
    "with open(gtnlplib.constants.TRAINKEY, 'r') as f_idxes:\n",
    "    root_folder = gtnlplib.constants.TRAINKEY.split('/')[0]\n",
    "    for name_label in f_idxes:\n",
    "        filename, label = name_label.split(' ')\n",
    "        file_prefix = filename.split('/')[1].split('.')[0]\n",
    "        filename = root_folder + '/' + filename\n",
    "        label = label.strip()\n",
    "        with open(filename.strip(), 'r') as f:\n",
    "            for line in f:\n",
    "                decoded = line.decode('ascii','ignore')\n",
    "                sentences = sent_tokenize(decoded)\n",
    "                s_idx = 0\n",
    "                for s in sentences:\n",
    "                    words = word_tokenize(s)\n",
    "                    cnt = 0\n",
    "                    for w in words:\n",
    "                        if w.isalpha():\n",
    "                            cnt += 1\n",
    "                    if cnt == 0:\n",
    "                        continue                    \n",
    "                    subfile = 'train_sentences/%s_%d.txt' % (file_prefix, s_idx)\n",
    "                    with open('data/' + subfile, 'w') as subf:\n",
    "                        subf.write(s)\n",
    "                    fs_key.write(subfile + ' ' + label + '\\n')\n",
    "                    s_idx += 1\n",
    "fs_key.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('data/train_crop'):\n",
    "    shutil.rmtree('data/train_crop')\n",
    "os.mkdir('data/train_crop')\n",
    "\n",
    "times = 20\n",
    "fraction = 0.5\n",
    "fs_key = open('data/train_crop.key', 'w')\n",
    "with open(gtnlplib.constants.TRAINKEY, 'r') as f_idxes:\n",
    "    root_folder = gtnlplib.constants.TRAINKEY.split('/')[0]\n",
    "    for name_label in f_idxes:\n",
    "        filename, label = name_label.split(' ')\n",
    "        file_prefix = filename.split('/')[1].split('.')[0]\n",
    "        filename = root_folder + '/' + filename\n",
    "        label = label.strip()\n",
    "        with open(filename.strip(), 'r') as f:\n",
    "            for line in f:\n",
    "                decoded = line.decode('ascii','ignore')\n",
    "                sentences = sent_tokenize(decoded)\n",
    "                wlist = []\n",
    "                for s in sentences:\n",
    "                    words = word_tokenize(s)                    \n",
    "                    for w in words:\n",
    "                        if w.isalpha():\n",
    "                            wlist.append(w)\n",
    "                            \n",
    "                for s_idx in range(times):\n",
    "                    subfile = 'train_crop/%s_%d.txt' % (file_prefix, s_idx)\n",
    "                    with open('data/' + subfile, 'w') as subf:\n",
    "                        for i in range(len(wlist)):\n",
    "                            if len(wlist) < 10 or np.random.rand() > fraction:\n",
    "                                subf.write(wlist[i] + ' ')\n",
    "                        subf.write('\\n')\n",
    "                    fs_key.write(subfile + ' ' + label + '\\n')\n",
    "fs_key.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('data/train_randconcat'):\n",
    "    shutil.rmtree('data/train_randconcat')\n",
    "os.mkdir('data/train_randconcat')\n",
    "\n",
    "times = 1000\n",
    "fs_key = open('data/train_randconcat.key', 'w')\n",
    "sample_dict = {'POS' : [], 'NEG' : [], 'NEU' : []}\n",
    "\n",
    "with open(gtnlplib.constants.TRAINKEY, 'r') as f_idxes:\n",
    "    root_folder = gtnlplib.constants.TRAINKEY.split('/')[0]\n",
    "    for name_label in f_idxes:\n",
    "        filename, label = name_label.split(' ')\n",
    "        file_prefix = filename.split('/')[1].split('.')[0]\n",
    "        filename = root_folder + '/' + filename\n",
    "        label = label.strip()\n",
    "        with open(filename.strip(), 'r') as f:\n",
    "            for line in f:\n",
    "                decoded = line.decode('ascii','ignore')\n",
    "                sentences = sent_tokenize(decoded)\n",
    "                sample_dict[label].append(sentences)\n",
    "                wlist = []\n",
    "                for s in sentences:\n",
    "                    words = word_tokenize(s)\n",
    "                    for w in words:\n",
    "                        if w.isalpha():\n",
    "                            wlist.append(w)\n",
    "                subfile = 'train_randconcat/%s.txt' % file_prefix\n",
    "                with open('data/' + subfile, 'w') as subf:\n",
    "                    for i in range(len(wlist)):\n",
    "                        subf.write(wlist[i] + ' ')\n",
    "                    subf.write('\\n')\n",
    "                fs_key.write(subfile + ' ' + label + '\\n')\n",
    "                \n",
    "for label in sample_dict:\n",
    "    for t in range(times):\n",
    "        subfile = 'train_randconcat/%s_%d.txt' % (label, t)\n",
    "        idx1 = np.random.randint(len(sample_dict[label]))\n",
    "        idx2 = np.random.randint(len(sample_dict[label]))\n",
    "        if idx1 == idx2:\n",
    "            continue\n",
    "        s1 = sample_dict[label][idx1]\n",
    "        s2 = sample_dict[label][idx2]\n",
    "        fs_key.write(subfile + ' ' + label + '\\n')\n",
    "        with open('data/' + subfile, 'w') as subf:\n",
    "            wlist = []\n",
    "            for i in range(len(s1) / 2 + 1):\n",
    "                subf.write(s1[i] + ' ')\n",
    "            for i in range(len(s2) / 2 + 1):\n",
    "                subf.write(s2[i] + ' ')\n",
    "            subf.write(' '.join(wlist) + '\\n')\n",
    "fs_key.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
