{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6. Distributed Embeddings and Semi-Supervised Learning #\n",
    "\n",
    "In this assignment, you will use singular value decomposition (SVD) as well as Word2Vec to learn about lexical semantics. You will have to work with \"big\" data\n",
    "- big enough that you will have to think carefully about speed and memory. Of particular importance will **sparse** matrix\n",
    "representations of your data. For this problem you will be submitting pdf version of ipython with outputs along with original ipython. Some particular functions and classes you might need:\n",
    "\n",
    "- [scipy.sparse.csr_matrix](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) - matrix in compressed sparse row format\n",
    "- [scipy.sparse.diags](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) - method for creating sparse diagonal matrices\n",
    "- [diagonal()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html) - get the diagonal of a matrix\n",
    "- [sklearn.preprocessing.normalize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) - efficiently normalize sparse matrices\n",
    "- [scipy.sparse.csr_matrix.asfptype()](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.asfptype.html) - upcast matrix to a floating point format\n",
    "- [scipy.cluster.vq.kmeans2](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html#scipy.cluster.vq.kmeans2) - Classify a set of observations into k clusters using the k-means algorithm\n",
    "- [numpy.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) - returns the indices that would sort an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import hstack, diags, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import csv\n",
    "from word2vec import VectorModel\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csv2csr(filename):\n",
    "    word = []\n",
    "    context = []\n",
    "    count = []\n",
    "    with open(filename,'rb') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            word.append(int(row[0]))\n",
    "            context.append(int(row[1]))\n",
    "            count.append(int(row[2]))\n",
    "    return csr_matrix((count,(word,context)))\n",
    "\n",
    "def readVocab(filename):\n",
    "    vocab = []\n",
    "    with open(filename,'rb') as vocabfile:\n",
    "        for line in vocabfile:\n",
    "            vocab.append(line.split()[0])\n",
    "    index = dict(zip(range(0,len(vocab)),vocab)) #from numbers to words\n",
    "    inv_index = {j:i for i,j in index.items()} #from words to numbers\n",
    "    return index,inv_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data ##\n",
    "\n",
    "Call **C=proj4_starter.csv2csr('doc_trips.csv')** to load a sparse matrix $C$ of a word-document counts. The cell $c[i,j]$ should \n",
    "hold the count of word $i$ in document $j$.\n",
    "\n",
    "Call **idx, iidx=proj4_starter.readVocab('vocab.10k')** to load the vocabulary. You get two **dict** objects, mapping between words \n",
    "and indices in the matrix $C$. In **C[iidx['Obama'],:]**, you have the document counts for the word *Obama*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = csv2csr('doc_trips.csv')\n",
    "idx, iidx = readVocab('vocab.10k')\n",
    "print C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cosine Similarity ##\n",
    "\n",
    "The *cosine similarity* of two vectors $u$ and $v$ is defined as \n",
    "$$\\frac{\\sum_{i}u_iv_i}{\\sqrt{\\sum_iu_i^2\\sum_iv_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2a** Consider the words *coffee, play, crazy, facebook*, and *hermana* (Spanish for *sister*). For each of them, find the 10 most similar words according to cosine similarity of the rows in $C$. (2 points)\n",
    "\n",
    "**Hint** The size of the vocabulary is nearly 10,000 words. You do not want to compute and store the entire $10K\\times 10K$ matrix \n",
    "of cosine similarities. Rather, you want to compute them on demand for a given row of the matrix. You may also want to do some\n",
    "precomputation to take care of denominator in advance. Whatever you do, don't lose the sparsity of $C$, or you will not be able \n",
    "to store it.\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words I get are *facebook page on twitter deleted instagram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is the word list\n",
    "word_list = ['coffee','play','crazy','facebook','hermana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalizeRow(x):\n",
    "    '''\n",
    "    Normalize each row of x\n",
    "    '''\n",
    "    return diags(np.array(1./(1e-6+np.sqrt(x.multiply(x).sum(axis=1))))[:,0],0) * x\n",
    "\n",
    "def computeCosSimPerWord(word_idx, x):\n",
    "    '''\n",
    "    For a given data matrix, compute cosine similarity between the word with index \"word_index\" and all words \n",
    "    (including itsself)\n",
    "    \n",
    "    Should return a 1-D np.array, not a matrix.\n",
    "    '''\n",
    "    \n",
    "    pass\n",
    "\n",
    "normalizedC = normalizeRow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSimilarWords  is used to print the top 10 similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printSimilarWords(x, word_list, sim_func, vocab=idx, ivocab=iidx):\n",
    "    for word in word_list:\n",
    "        print word, ':', \n",
    "        word_idx = ivocab[word]\n",
    "        sim_idx = np.argsort(-sim_func(word_idx, x))[:10]\n",
    "        for word2_idx in sim_idx:\n",
    "            print vocab[word2_idx],\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSimilarWords(normalizedC, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2b ** Come up with five words of your own that you think might be interesting, and list the top 10 most similar for each. Try to choose a few different types of words, such as verbs, adjectives, names, emotions, abbreviations, or alternative spellings. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSimilarWords(normalizedC, additional_word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Co-occurence ##\n",
    "\n",
    "Compute the document co-occurence matrix $D$, where $d_{i,j}$ is the probability $P(w_j|w_i)$ that word $j$ appears in a tweet, \n",
    "given that word $i$ appears. To do this, first compute the co-occurence counts $CC^\\top$. Substract the diagonal, then normalize \n",
    "each row. \n",
    "\n",
    "Note: it is possible to smooth this probability, but if you naively add some number to the matrix, you will lose sparsity \n",
    "and memory will blow up. You can do it unsmoothed. However, smoothing is not required here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3** For each of the 10 examples above (my five words and your five words), find the 10 most similar words according to cosine similarity of the rows of $D$. (2 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the 5 words I get are *facebook instagram twitter tv youtube*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeCooccurMatrix(C):\n",
    "    '''\n",
    "    Compute the co-occurence matrix D\n",
    "    '''\n",
    "    D = None\n",
    "    return D\n",
    "\n",
    "D = computeCooccurMatrix(C)\n",
    "normalizedD = normalizeRow(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = word_list + additional_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSimilarWords(normalizedD, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Semantic Analysis ##\n",
    "\n",
    "Perform truncated SVD (**scipy.sparse.linalg.svds**) to obtain $USV^\\top\\approx C$ using $K=10$. Each row vector $u_i$\n",
    "is a description of the word $i$. You can compute similarity between pairs of words using the squared Euclidean norm \n",
    "$\\|u_i-u_j\\|^2_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(a)** For each of the 10 examples above, find the 10 most similar words according to squared Euclidean distance in $U$. (3 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words are *facebook ex harry calls snap *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeEuclidDist(word_idx, U,with_S = False):\n",
    "    '''\n",
    "    Compute the Euclid distance bwteen word with index \"word_idx\" and all words \n",
    "    (including itself)\n",
    "    \n",
    "    Args:\n",
    "      word_idx - the word index\n",
    "      U - latent representation of words\n",
    "    Return:\n",
    "        Euclidean distance from representation of word_idx to all words\n",
    "    '''\n",
    "    dis = 0\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Once you finish the function computeEuclidDist, run the following code directly to print results\n",
    "'''\n",
    "Cfp= C.asfptype()\n",
    "U = svds(Cfp, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(b) ** Now compute the same SVD with $K=50$, and again find the 10 most similar words according to Euclidean distance $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(c) ** Now compute the SVD of the matrix $\\mathbf{D}$, using with $K = 10$, and $K = 50$. Report \n",
    "the most similar words to each of the example words according to Euclidean distance in $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here for K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here for K = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optionally, try K = 100 and see if it's even better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local Context ##\n",
    "\n",
    "Local context captures the frequency with which words appear in each others’ immediate context. We have provided a CSV file (succ_trips_50k.csv)  in which each line contains a triple $\\langle x,y,z\\rangle$, \n",
    "where $x$ and $y$ are term IDs and $z$ is the count of times where $y$ immediately follows $x$ . \n",
    "The vocabulary has now increased to 50K words. There is an associated vocabulary file, **vocab.50k**.\n",
    "\n",
    "**Deliverable 5a **\n",
    "Build a sparse matrix $\\mathbf{E}$ from these triples. Normalize the rows of $\\mathbf{E}$, such that $e_{i,j}=\\frac{n(i,j)}{n(i)}$, the probability of seeing word $j$ given that you have just seen word $i$. \n",
    "Now form a matrix $\\mathbf{F} = [\\mathbf{E}~ \\mathbf{E}’]$ by horizontally concatenating the normalized matrix $\\mathbf{E}$. You will perform sparse singular value decomposition on $\\mathbf{F}$. (2 points)\n",
    "\n",
    "**Hint** make sure you are using a sparsity-preserving operation to combine E and E'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_50, iidx_50 = readVocab('vocab.50k')\n",
    "E = csv2csr('succ_trips_50k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def constructF(E):\n",
    "    '''\n",
    "    Finish the following code to construct F from E\n",
    "    '''\n",
    "   \n",
    "    Enorm =None\n",
    "    return Enorm\n",
    "    \n",
    "F = constructF(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 5b ** For $K = 10$ and $K = 50$ compute the top 10 synonyms for each of your ten words. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K = 10\n",
    "U_f,S_f,V_f = svds(F, 10)\n",
    "# note that we have to insert the new vocabulary as an optional argument\n",
    "printSimilarWords(U_f, word_list, vocab=idx_50, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code for K = 50 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5c ** Overall, which set of synonyms looks best to you? \n",
    "Count how many of the top 5 synonyms for *coffee* and   *crazy*\n",
    "have the same majority part of speech (e.g., *play* is a verb) as the cue word.\n",
    "Use the tagset from the [Twitter POS paper](http://www.cc.gatech.edu/~jeisenst/papers/acl2012pos.pdf). Does local context or document context do better at matching the POS of the cue words? Why? (Consider first context provided by wordnet as majority POS). (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec ##\n",
    "As mentioned in the class Word2Vec are distributed continuous vector representations  for words.\n",
    "In this part, you will be building and training Word2Vec module keeping in mind various hyper parameters such as min_count, window_size etc.(10 points)\n",
    "You will be using Gensim http://radimrehurek.com/gensim/ to train your model.\n",
    "\n",
    "- For instructions on how to install gensim, see [here](https://radimrehurek.com/gensim/install.html). \n",
    "- I strongly recommend that you test your install by downloading the [source](http://pypi.python.org/pypi/gensim) and running ```python setup.py test```\n",
    "- For a tutorial on how to use gensim for word embeddings, see [here](http://rare-technologies.com/word2vec-tutorial/)\n",
    "- It is recommended that you have a working c compiler, so that the much faster cythonized gensim can run. This is transparent to you, but does require that you have a c compiler installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "        print \"Opening the file...\"\n",
    "\n",
    "        X_train = []\n",
    "\n",
    "        f = open(filename,'r')\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            sentence = []\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                sentence = word_tokenize(line)\n",
    "            except:\n",
    "                pass\n",
    "            if(len(sentence) > 2):\n",
    "                count =count+1\n",
    "                X_train.append(array(sentence))\n",
    "            # else:\n",
    "            #      print \"No words\"\n",
    "            #      print sentence\n",
    "\n",
    "        print \"File successfully read\"\n",
    "        print count , \"sentences\"\n",
    "        f.close()\n",
    "        return array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = read_data('wiki.train.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build and train model you will be using [Gensim](http://radimrehurek.com/gensim/). \n",
    "\n",
    "This [tutorial](https://radimrehurek.com/gensim/models/word2vec.html) may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6a**: Build a model with embedding size = 10, and build a vocabulary from the data, with min_count = 10. Print the vocabulary size, which should be between 9000 and 10000. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6b**: Train your model for one iteration, and report the similarity between the words (\"cat\" and \"dog\"), (\"night\" and \"day\"), and (\"can\" and \"fish\"). (2 points)\n",
    "\n",
    "**Hint**: before training on the whole dataset, train on sentences[0:5] to make sure it works and isn't too slow. On my laptop, one iteration takes 2-3 seconds. \n",
    "\n",
    "**Sanity check** the similarity I get between \"cat\" and \"dog\" is 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6c** Modify the code below to compare the performance of different embedding sizes. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainMod(model,sentences,max_its=10):\n",
    "    # your code to build the vocabulary\n",
    "    scores = []\n",
    "    times = []\n",
    "    for _ in xrange(max_its):\n",
    "        # your code to train the model\n",
    "        scores.append(sum(model.score(sentences)))\n",
    "        times.append(model.total_train_time)\n",
    "    return scores,times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_k = [20,40,100] # embedding sizes\n",
    "models = []\n",
    "for k in all_k:\n",
    "    model = #your code to create a model here\n",
    "    models.append(model)\n",
    "    scores,times = trainMod(model,sentences)\n",
    "    plt.plot(times,scores,'o-');\n",
    "    model.save('model-%d'%(k))\n",
    "    print 'done with',k\n",
    "plt.legend(['k=%d'%(k) for k in all_k],loc='lower right');\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6d** Use the ```most_similar``` function to find similar words to the items in your word_list which are in the vocabulary for your word2vec models. Compare these most similar words with the outputs of the methods tried earlier in the assignment. You may train the word2vec models for longer if you wish, or change any of the parameters. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(your explanation here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the claims about word2vec word embeddings is that they can solve analogy problems, like \"man:woman::king:queen\"\n",
    "\n",
    "[This tutorial](http://rare-technologies.com/word2vec-tutorial/) contains example code showing how to use word vectors to try to solve analogies.\n",
    "\n",
    "**Deliverable 6e** Using the three models trained in the previous deliverable, see if each can solve the analogies man:woman::king:queen and architect:building::painter:painting. Invent two more analogies, and see which of the three models can solve them. You can Google analogies, but you may have to search for a while to find analogies where all four elements are in the vocabulary. You can also retrain the models with a larger vocabulary if you want. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Semi-supervised learning #\n",
    "\n",
    "Now you will use the word embeddings to try to improve your dependency parser from problem set 5.\n",
    "\n",
    "You will do this by clustering words according to their embedding.\n",
    "\n",
    "The code below run the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose a model to use here\n",
    "word_vectors = model.syn0\n",
    "# run the clustering algorithm\n",
    "centroids, labels = kmeans2(word_vectors,num_clusters,iter=100,minit='points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 7a** Modify the code below to show the words in the same cluster as a query word. Find the words in the same cluster as \"coffee\", \"computer\", and \"red\" for at least one of your models, plus any other words of interest. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsInSameCluster(word,model,labels):\n",
    "    # you will use model.index2word to make this function\n",
    "    # return a list of words in the same cluster as word\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving dependency parsing ##\n",
    "\n",
    "Now incorporate these cluster features into your best performing parser from project 5, based on the dev data. Add a feature for each cluster/tag pair, e.g., C175/N, C189/V, etc. You will then compute the accuracy with training sets of various sizes, comparing the performance of your model with and without the cluster features.\n",
    "\n",
    "**Deliverable 7b ** Build training sets including the first 50, 100, 200, 500, and 1000 *sentences* (not words). \n",
    "Train your tagger on each training set, using your original features, and plot the accuracy on the development set. \n",
    "Then retrain you tagger, including the new word cluster features, and plot accuracy on the development set on the same plot. \n",
    "Run for at least 10 iterations in each case.\n",
    "\n",
    "You may want to try larger numbers of clusters to improve performance. \n",
    "\n",
    "You can even include the results from multiple clusterings with different numbers of clusters (50, 100, 200, 500, ...), \n",
    "using features like C43-50/N (cluster 43 of 50, with tag N), C377-500/V (cluster 377 of 500, with tag V).\n",
    "\n",
    "You can also use clusters for the features of neighboring words.  (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Train 100'\n",
    "dp = depp.DependencyParser(feature_function=BakeOffFeats())\n",
    "dp.read_data(\"english\",100)\n",
    "tr_acc ,dv_acc  = dp.train_perceptron(10) \n",
    "\n",
    "print 'Train 100'\n",
    "dp = depp.DependencyParser(feature_function=W2V_ClusterFeats())\n",
    "dp.read_data(\"english\",100)\n",
    "tr_acc ,dv_acc  = dp.train_perceptron(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Get creative ##\n",
    "\n",
    "This part is mandatory for 7650 and optional for 4650.\n",
    "\n",
    "**Deliverable 8 ** K-means clustering is one way to find similarity. This part is opened-ended and requires you to come up with creative solutions of using distributed vectors to improve the results. In addition to implementing your idea, provide explanation of what you did, and why you thought it should work.\n",
    "\n",
    "The top 3 selected ideas (based mostly on performance, but also on creativity) will get an extra point towards their final score.(5 points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
